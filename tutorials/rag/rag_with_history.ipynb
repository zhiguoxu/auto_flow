{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Chain without chat history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from core.llm.openai.openai_llm import OpenAILLM\n",
    "\n",
    "llm = OpenAILLM(model=\"gpt-4o\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from core.rag.splitter.text_splitter import TextSplitter\n",
    "from core.rag.vectorstore.chroma import Chroma\n",
    "from core.rag.document_loaders.dir_loader import DirLoader\n",
    "\n",
    "docs = DirLoader(file_or_dir=\"./files/paul_graham_essay.txt\").load()\n",
    "docs = TextSplitter().split_document(docs)\n",
    "vectorstore = Chroma()\n",
    "vectorstore.add_documents(docs)\n",
    "retriever = vectorstore.as_retriever(top_k=10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from core.rag.document.document import Document\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def format_docs(documents: List[Document]):\n",
    "    return \"\\n\\n\".join(doc.text for doc in documents)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from core.llm.message_parser import StrOutParser\n",
    "from core.rag.prompts import qa_prompt\n",
    "from core.flow.flow import identity\n",
    "\n",
    "rag_flow = (\n",
    "        {\"context\": retriever | format_docs, \"question\": identity}\n",
    "        | qa_prompt\n",
    "        | llm\n",
    "        | StrOutParser()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rag_flow.invoke(\"what did paul graham do after going to RISD?\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Contextualizing the question"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from core.rag.retrivers.history_aware_retriever import create_history_aware_retriever\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(llm, retriever)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Chain with chat history"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from core.rag.helpers import create_rag_flow\n",
    "from core.rag.combine_documents.stuff import create_stuff_documents_flow\n",
    "from core.prompts.chat_template import ChatTemplate, MessagesPlaceholder\n",
    "\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_answer_flow = create_stuff_documents_flow(llm, qa_prompt)\n",
    "rag_flow = create_rag_flow(history_aware_retriever, question_answer_flow)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from core.messages.chat_message import ChatMessage, Role\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"what did paul graham do after going to RISD?\"\n",
    "ai_msg_1 = rag_flow.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([ChatMessage(role=Role.USER, content=question), ai_msg_1[\"answer\"]])\n",
    "\n",
    "second_question = \"Where dit he work after that?\"\n",
    "ai_msg_2 = rag_flow.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
